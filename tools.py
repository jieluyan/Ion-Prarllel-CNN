import os
import random
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
from random import seed, randint, sample
import numpy as np
import pandas as pd
from pandas import ExcelWriter
import matplotlib.pyplot as plt
from ifeature.codes import *
from ifeature.PseKRAAC import *
from pycaret.utils import check_metric
from sklearn.metrics import confusion_matrix
from tensorflow import keras
from tensorflow.keras import layers, Sequential, losses, metrics, models
from tensorflow.keras.models import Model
import tensorflow as tf
best18fts_cdhit = ["type3Braac9", "type7raac19", "type8raac16", "type7raac18", "type1raac18", "type10raac18",
                   "type8raac14", "CKSAAP", "DDE", "type7raac17", "type11raac10", "type12raac16", "type12raac17",
                   "type8raac13", "KSCTriad", "type7raac16", "type1raac19", "type10raac19"]

def getIonRootLocation():
    cur_dir = os.path.dirname(os.path.realpath(__file__))
    # root_dir = '/'.join(cur_dir.split('/')[:-1])
    root_dir = cur_dir
    return root_dir

def createFolder(folder):
    if not os.path.isdir(folder):
        os.makedirs(folder)
        print("created folder: \n\t %s" % folder)
    else:
        print("%s existed." % folder)
    return

def executeCdhit(input, output, c=0.4, n=2):
    cmd = 'cd-hit -c %.1f -n %d -i %s -o %s' % (c, n, input, output)
    msg = os.popen(cmd).read()
    print(msg)
    return msg

def cdHitByInput(abs_input, c=0.8, n=2, redo=True):
    in_folder = "/".join(abs_input.split("/")[:-1])
    in_name = abs_input.split("/")[-1].split(".")[0]
    out_folder = "cd-hit-output/c%dn%d" % (c*10, n)
    out_name = "%s_c%dn%d.fasta" % (in_name,c*10, n)
    abs_out_folder = os.path.join(in_folder,out_folder)
    createFolder(abs_out_folder)
    abs_output = os.path.join(abs_out_folder, out_name)
    msg = ''
    path_flag = os.path.isfile(abs_output)
    if (path_flag and redo) or (not path_flag):
        if path_flag:
            print("%s\n\talready existed and it will be replaced by"
                  "\n\t\tcodes.split_dataset_tools.cdHitByInput\n" % abs_output)
        else:
            print("%s\n\tnot existed and it will be generated by"
                  "\n\t\tcodes.split_dataset_tools.cdHitByInput\n" % abs_output)
        msg += executeCdhit(abs_input, abs_output, c=c, n=n)
    else:
        print("%s\n\talready existed and nothing will be implement by"
              "\n\t\tcodes.split_dataset_tools.cdHitByInput\n" % abs_output)

    print(msg)
    return abs_output

def executeCdhit2d(pos_in, neg_in, neg_out, c=1, n=2, redo=True):
    path_flag = os.path.isfile(neg_out)
    msg = ''
    if (path_flag and redo) or (not path_flag):
        if path_flag:
            print("%s\n\talready existed and it will be replaced by"
                  "\n\t\tcodes.split_dataset_tools.executeCdhit2d\n" % neg_out)
        else:
            print("%s\n\tnot existed and it will be generated by"
                  "\n\t\tcodes.split_dataset_tools.executeCdhit2d\n" % neg_out)
        cmd = 'cd-hit-2d -c %.1f -n %d -i %s -i2 %s -o %s' % \
              (c, n, pos_in, neg_in, neg_out)
        msg = os.popen(cmd).read()
        print(msg)
    else:
        print("%s\n\talready existed and nothing will be implement by"
              "\n\t\tcodes.split_dataset_tools.executeCdhit2d\n" % neg_out)
    return msg

def findCluster(clstr_path):
    sub_clstr = []
    clstrs = []
    c = 0
    with open(clstr_path, "r") as f:
        for line in f.readlines():
            if not(">Cluster" in line):
                # uniprot: >Cluster 0
                # '0\t26aa, >P37300|ori... at 42.31%'
                # kaliumdb: >Cluster 0
                # 0	223aa, >Q91055... *
                name = line.split('...')[0].split(">")[-1].split('|')[0]
                # print(name)
                sub_clstr.append(name)
            if ">Cluster" in line:
                clstrs.append(sub_clstr)
                sub_clstr = []
        clstrs.append(sub_clstr)
    return clstrs[1:]

# keepTestXpercentToTestAddTrain
def split2sets(clstrs, x=10):
    train_names, test_names, novel_names = [], [], []
    seed(1)
    # c = 0
    for sub_clstr in clstrs:
        clstr_no = len(sub_clstr)
        if clstr_no == 1:
            novel_names.extend(sub_clstr)
            # print("novel", novel_names)
        elif clstr_no > 1:
            rnd_ind = randint(0, clstr_no-1)
            # print("Cluster: ", c)
            test_name = sub_clstr[rnd_ind]
            test_names.append(test_name)
            # print(test_name)
            train_names.append(sub_clstr[:rnd_ind])
            train_names.append(sub_clstr[rnd_ind+1:])
        # c += 1
    train_names = [item for sublist in train_names for item in sublist]
    train_names, test_names = keepTestXpercentToTestAddTrain(train_names, test_names, x=x)
    return train_names, test_names, novel_names

def keepTestXpercentToTestAddTrain(train_names, test_names, x=10):
    """
    :param train_names: given train names list
    :param test_names: given test names list
    :param x: is the desired_ratio = desired_test_len / (desired_test_len + desired_train_len) * 100;
    real_ratio = test_len / (test_len + train_len) * 100;
    :return: if x == 0 or the given desired_ratio == real_ratio, then do nothing and return train_names and test_names;
    else change train_names and test_names make them to the desired ratio
    """
    test_len, train_len = len(test_names), len(train_names)
    train_names, test_names = np.array(train_names), np.array(test_names)
    test_len_to = int((test_len + train_len) * x /100)
    if x == 0:
        return train_names.tolist(), test_names.tolist()
    if test_len_to == 0:
        test_len_to = 1
    if test_len_to > test_len:
        add_test_num = test_len_to - test_len
        # select add_test_num from train add them to test
        add_list = sample(list(range(train_len)), add_test_num)
        train_ind = [True] * train_len
        for i in add_list:
            train_ind[i] = False
        new_train = train_names[train_ind]
        rm_train = [train_names[i] for i in add_list]
        new_test = np.append(test_names, rm_train)
    elif test_len_to < test_len:
        rm_test_num = test_len - test_len_to
        # romve rm_test_num test elements and add them to train
        rm_list = sample(list(range(test_len)), rm_test_num)
        test_ind = [True] * test_len
        for i in rm_list:
            test_ind[i] = False
        new_test = test_names[test_ind]
        rm_test = [test_names[i] for i in rm_list]
        new_train = np.append(train_names, rm_test)
    else:
        new_train = train_names
        new_test = test_names
    return new_train.tolist(), new_test.tolist()

def getSeqRecords(fasta_path, train_names, test_names, novel_names):
    train_set, test_set, novel_set = [], [], []
    for record in SeqIO.parse(fasta_path, "fasta"):
        # get sequence name record.id = "P61542|ori"
        name = record.id.split("|")[0]
        record.id = name
        if name in novel_names:
            novel_set.append(record)
        if name in test_names:
            test_set.append(record)
        if name in train_names:
            train_set.append(record)
    return train_set, test_set, novel_set

def findPathOfTrainTestNovelDataset(fasta_folder, fasta_name, clstr_folder, clstr_name, redo=False, x=10):
    msg = ""
    root_dir = getIonRootLocation()
    folder = os.path.join(root_dir, fasta_folder)
    cls_folder = os.path.join(root_dir, clstr_folder)
    fasta_path = os.path.join(folder, fasta_name)
    clstr_path = os.path.join(cls_folder, clstr_name)

    act_name = fasta_name.split(".")[0]
    out_folder = os.path.join(cls_folder, 'split_dataset', act_name)
    # create output folder of "split_dataset"
    createFolder(out_folder)
    # act_name = fasta_name.split(".")[0]
    novel_path = os.path.join(out_folder, "novel_" + fasta_name)
    test_path = os.path.join(out_folder, "test_" + fasta_name)
    train_path = os.path.join(out_folder, "train_" + fasta_name)
    return msg, novel_path, test_path, train_path

# fasta_folder: folder location of the project (ion_channels)
# clstr_folder = 'uniprot-preprocessed-dataset/cd-hit-output/c4n2'
# fasta_name = 'calcium.fasta'
# clstr_name = 'calcium.fasta.clstr'
def generateTrainTestNovelDataset(fasta_folder, fasta_name, clstr_folder, clstr_name, redo=False, x=10):
    root_dir = getIonRootLocation()
    folder = os.path.join(root_dir, fasta_folder)
    cls_folder = os.path.join(root_dir, clstr_folder)
    fasta_path = os.path.join(folder, fasta_name)
    clstr_path = os.path.join(cls_folder, clstr_name)

    act_name = fasta_name.split(".")[0]
    out_folder = os.path.join(cls_folder, 'split_dataset', act_name)
    # create output folder of "split_dataset"
    createFolder(out_folder)
    # act_name = fasta_name.split(".")[0]
    novel_path = os.path.join(out_folder, "novel_" + fasta_name)
    test_path = os.path.join(out_folder, "test_" + fasta_name)
    train_path = os.path.join(out_folder, "train_" + fasta_name)
    msg = 'All the required files existed, nothing to do.\n\tfrom codessplit_dataset_tools.generateTrainTestNovelDataset'

    path_flag = os.path.isfile(test_path)
    if (path_flag and redo) or (not path_flag):
        if path_flag:
            print("%s\n\talready existed and it will be replaced by"
                  "\n\t\tcodes.split_dataset_tools.generateTrainTestNovelDataset"
                  "\n\t\tso as to trian novel set \n" % test_path)
        else:
            print("%s\n\tnot existed and it will be generated by"
                  "\n\t\tcodes.split_dataset_tools.generateTrainTestNovelDataset"
                  "\n\t\tso as to trian novel set \n" % test_path)
        clstrs = findCluster(clstr_path)
        train_names, test_names, novel_names = split2sets(clstrs, x=x)
        # names, seqs, lens, records = readFastaYan(fasta_path)
        train_set, test_set, novel_set = getSeqRecords(fasta_path, train_names, test_names, novel_names)
        SeqIO.write(novel_set, novel_path, 'fasta')
        SeqIO.write(test_set, test_path, 'fasta')
        SeqIO.write(train_set, train_path, 'fasta')
        msg = "The dataset was split into 3 subsets by clstr file, they are: "
        msg += "\n\t%s" % novel_path
        msg += "\n\tsequence number: %d" % len(novel_set)
        msg += "\n\t%s" % test_path
        msg += "\n\tsequence number: %d" % len(test_set)
        msg += "\n\t%s" % train_path
        msg += "\n\tsequence number: %d" % len(train_set)
    else:
        print("%s\n\talready existed and nothing will be implemented by"
              "\n\t\tcodes.split_dataset_tools.generateTrainTestNovelDataset"
              "\n\t\tso as to trian novel set \n" % test_path)
    print(msg)
    return msg, novel_path, test_path, train_path

def findPathOfCdhitAndSplitDataset(ori_path, c=0.4, n=2, redo=False, x=10):
    root_dir = getIonRootLocation()
    fasta_folder = os.path.dirname(ori_path)
    fasta_name = ori_path.split("/")[-1]
    folder = os.path.join(root_dir, fasta_folder)
    fasta_path = os.path.join(folder, fasta_name)
    #clstr_path = os.path.join(folder, clstr_name)
    out_folder_name = "c%dn%d" % (c*10, n)
    # set output fasta name after cd hit process
    output_fasta = fasta_name.split('.')[0] + '_cdhit_' + out_folder_name + '.fasta'
    # gene clstr name of the corresponding fasta  after cd hit process
    output_clstr = output_fasta + '.clstr'
    out_folder = os.path.join(folder, 'cd-hit-output', out_folder_name)
    out_fasta_path = os.path.join(out_folder, output_fasta)
    # generate cd hit fasta and clstr files of the given file fasta_path
    msg = ""
    out_fasta_folder = os.path.join(fasta_folder, 'cd-hit-output', out_folder_name)
    temp_msg, novel_path, test_path, train_path = findPathOfTrainTestNovelDataset(fasta_folder, fasta_name, out_fasta_folder, output_clstr, redo=redo, x=x)
    return msg, out_folder, novel_path, test_path, train_path

# the fasta folder is the folder corresponding to ion-channel root folder
# fasta_folder = 'uniprot-preprocessed-dataset'
# fasta_name = 'calcium.fasta'
def executeCdhitAndSplitDataset(ori_path, c=0.4, n=2, redo=True, x=10):
    root_dir = getIonRootLocation()
    fasta_folder = os.path.dirname(ori_path)
    fasta_name = ori_path.split("/")[-1]
    folder = os.path.join(root_dir, fasta_folder)
    fasta_path = os.path.join(folder, fasta_name)
    #clstr_path = os.path.join(folder, clstr_name)
    out_folder_name = "c%dn%d" % (c*10, n)
    # set output fasta name after cd hit process
    output_fasta = fasta_name.split('.')[0] + '_cdhit_' + out_folder_name + '.fasta'
    # gene clstr name of the corresponding fasta  after cd hit process
    output_clstr = output_fasta + '.clstr'
    out_folder = os.path.join(folder, 'cd-hit-output', out_folder_name)

    # create output folder of "split_dataset"
    createFolder(out_folder)

    out_fasta_path = os.path.join(out_folder, output_fasta)
    # generate cd hit fasta and clstr files of the given file fasta_path
    msg = '\n\n ***** ***** processing: \n\t %s \n' % fasta_path + \
          '***** generating training testing and novel datasets from the above file ***** \n\n'

    path_flag = os.path.isfile(out_fasta_path)
    if (path_flag and redo) or (not path_flag):
        if path_flag:
            print("%s\n\talready existed and it will be replaced by cd-hit -c 0.4"
                  "\n\t\tcodes.split_dataset_tools.executeCdhitAndSplitDataset"
                  "\n\t\tso as to trian novel set \n" % out_fasta_path)
        else:
            print("%s\n\tnot existed and it will be generated by cd-hit -c 0.4"
                  "\n\t\tcodes.split_dataset_tools.executeCdhitAndSplitDataset"
                  "\n\t\tso as to trian novel set \n" % out_fasta_path)
        msg += executeCdhit(fasta_path, out_fasta_path, c=c, n=2)
    else:
        print("%s\n\talready existed and nothing will be implemented by cd-hit -c 0.4"
              "\n\t\tcodes.split_dataset_tools.executeCdhitAndSplitDataset"
              "\n\t\tso as to trian novel set \n" % out_fasta_path)
    print(msg)
    # generate 3 dataset from the given fasta_path and the corresponding clstr file
    # !!!! read this and recode again there is something wrong in file paths
    out_fasta_folder = os.path.join(fasta_folder, 'cd-hit-output', out_folder_name)

    temp_msg, novel_path, test_path, train_path = generateTrainTestNovelDataset(fasta_folder, fasta_name, out_fasta_folder, output_clstr, redo=redo, x=x)
    msg += temp_msg
    return msg, out_folder, novel_path, test_path, train_path

def randomGeneNegaFromPosFasta(in_fasta, out_fasta):
    in_names, in_seqs, in_lens, in_records = readFastaYan(in_fasta)
    natural_aa = ["A", "R", "N", "D", "C", "Q", "E", "G", "H", "I", "L", "K", "M", "F", "P", "S", "T", "W", "Y", "V"]
    out_records = []
    c = 0
    for l in in_lens:
        out_seq = ""
        c += 1
        for i in range(l):
            out_seq += random.sample(natural_aa,1)[0]
        name = "Neg%d" % c
        des = "length: %d" % l
        out_seq = Seq(out_seq)
        out_records.append(SeqRecord(out_seq, name, name, des))
    SeqIO.write(out_records, out_fasta, "fasta")
    return in_records, out_records

def shuffleGeneNegaFromPosFasta(in_fasta, out_fasta):
    in_names, in_seqs, in_lens, in_records = readFastaYan(in_fasta)
    out_records = []
    c = 0
    for s in in_seqs:
        s = s.__str__()
        c += 1
        l = len(s)
        # print("seq: \n\t%s" % s)
        # print(type(s))
        # print("len: \n\t%d" % l)
        out_seq = ''.join(random.sample(s,l))
        name = "Neg%d" % c
        des = "length: %d" % l
        out_seq = Seq(out_seq)
        out_records.append(SeqRecord(out_seq, name, name, des))
    SeqIO.write(out_records, out_fasta, "fasta")
    return in_records, out_records

# unnatural amino acids "B", "J", "O", "U", "Z", "X"
def containUnnaturalAminoAcid(seq):
    unnatural_amino_acids = ["B", "J", "O", "U", "Z", "X"]
    flag = False
    if any((c in unnatural_amino_acids) for c in s):
        flag = True
    return flag

def dropDuplicateSeqFromFasta(fasta_file):
    record_seqs = list()
    ori = 0
    folder = "/".join(fasta_file.split("/")[:-1]) + '/'
    out_name = fasta_file.split("/")[-1].split(".")[0] + '_remove_dupliate_seqs.fasta'
    out_path = folder + out_name
    records = []
    for record in SeqIO.parse(fasta_file, 'fasta'):
        ori += 1
        #print(record.seq)
        if record.seq not in record_seqs:
            record_seqs.append(record.seq)
            records.append(record)
    SeqIO.write(records, out_path, 'fasta')
    print("Ori fasta sequence number: ", ori)
    print("Unique fasta sequence number: ",len(records))
    print("Original %s contain %d sequences." % (fasta_file, ori))
    print("After removing duplicated sequences, there are %d sequences remained in %s." % (len(records), out_path))
    return(out_name)

# bins = list(range(start, end, slice))
# list(range(0,90,5))
# [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85]
# return count in the length range of [ (0 ~ 5), (5 ~ 10), (10 ~ 15), ..., (80 ~ 85)]
def fastaLengthDistribution(lens,bins):
    a = plt.hist(lens, bins=bins)
    # plt.close()
    return a[0]

# distribution is count number in different length range of a bins
# [ (0 ~ 5), (5 ~ 10), (10 ~ 15), ..., (80 ~ 85)]
# bins = list(range(start,end,slice))
def extractSpecifiedLengthDistribution(lens, distribution, bins):
    lens = np.array(lens)
    inds = []
    for i in range(len(bins)-1):
        start = bins[i]
        end = bins[i+1]
        count = distribution[i]
        # if count == 0:
        #     inds.extend([])
        sub_ind, = np.where(np.logical_and(lens >= start, lens <= end))
        while True:
            try:
                sub_ind = sub_ind
                # print("sub_ind: ", sub_ind)
                rnd_ind = np.random.choice(sub_ind, int(count), replace=False)
                inds.extend(rnd_ind.tolist())
                break
            except ValueError:
                print("ValueError: a must be 1-dimensional.")
                print("sub_ind: ")
                print(sub_ind)
                print("count: ")
                print(count)
    return(inds)

def readFastaYan(fasta_file):
    seqs = []
    lens = []
    names = []
    records =[]
    for record in SeqIO.parse(fasta_file, "fasta"):
        # read Sequences
        seqs.append(record.seq)
        # get sequence length
        lens.append(len(record.seq))
        # get sequence name record.id = "P61542|ori"
        name = record.id.split("|")[0]
        names.append(name)
        record.id = name
        records.append(record)
    return names, seqs, lens, records

# bins = list(range(start, end, slice))
# list(range(0,90,5))
# [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85]
# return count in the length range of [ (0 ~ 5), (5 ~ 10), (10 ~ 15), ..., (80 ~ 85)]
def change2ndfastaDistributionTheSameWith1st(fasta1, fasta2, out_fasta2, start=0, end=90, slice=5, ratio=1):
    names1, seqs1, lens1, records1 = readFastaYan(fasta1)
    names2, seqs2, lens2, records2 = readFastaYan(fasta2)
    bins = list(range(start, end, slice))
    d1 = fastaLengthDistribution(lens1, bins)
    d2 = fastaLengthDistribution(lens2, bins)
    ratio_array = d2/d1
    ratio_array[np.isnan(ratio_array)] = np.inf
    # print(ratio_array)
    max_ratio = min(ratio_array)
    # print(d2/d1)
    # all counts of a specific lens range should be integer
    new_d2 = [ int(ratio * x) for x in d1 ]
    if ratio > max_ratio:
        for i in range(len(new_d2)):
            if new_d2[i] > d2[i]:
                new_d2[i] = d2[i]
    print("length bins :")
    print(bins)
    print("No. of ori file1: ")
    print(d1)
    print("No. of ori file2: ")
    print(d2)
    print("No. of output bins: ")
    print(new_d2)
    ind = extractSpecifiedLengthDistribution(lens2, new_d2, bins)
    # new_seqs = seqs2[ind]
    # new_names = names2[ind]
    new_records2 = [records2[i] for i in ind]
    SeqIO.write(new_records2, out_fasta2, 'fasta')
    # writeFasta(new_names, new_seqs, out_fasta2)
    print("The output file %s was extracted by: \n\t%s \nAnd its length distribution is the same with %s"\
          % (out_fasta2, fasta2, fasta1))
    print("The max distribution ratio value between the 2nd fasta and 1st fasta is: %.3f" % max_ratio)
    return ratio

# activity = ["sodium", "potassium", "calcium", "negative"]
# for a in activity[:-1]:
# changeNeg2PosLenDis(pos_name=a)
def changeNeg2PosLenDis(fasta_folder = "../uniprot-preprocessed-dataset/", \
                        out_folder = "keep-length-distribution/", \
                        pos_name = "sodium", neg_name = 'negative', ratio = 1):
    cur_dir = os.path.dirname(os.path.realpath(__file__))
    pos_path = os.path.join(cur_dir, fasta_folder, pos_name + '.fasta')
    neg_path = os.path.join(cur_dir, fasta_folder, neg_name + '.fasta')
    out_name = neg_name + "_keepSameDistributionOf_" + pos_name + '.fasta'
    out_path = os.path.join(cur_dir, fasta_folder, out_folder, out_name)
    change2ndfastaDistributionTheSameWith1st(pos_path, neg_path, out_path, ratio = ratio)
    # drawTwoSubPlots(lens, fig_path)
    return pos_path, out_path

from ifeature.codes import readFasta

subtype = {'g-gap': 0, 'lambda-correlation': 4}
def genePsekraac(path, ft_name="type1", raactype=2, subtype={'g-gap': 0, 'lambda-correlation': 4}, ktuple=2, gap_lambda=1, class_val=1):
    """
    run codes as follows if you want try all kraac features of ifeature.PseKRAAC
    from codes.ifeature.PseKRAAC import *
    from codes.ifeature.codes import readFasta
    for ft_name in kraac:
        AAGroup = eval("%s.AAGroup" % ft_name)
        for raactype in AAGroup:
    :param path: given fasta file location
    :param ft_name: one element of the following list:
        ["type1","type2","type3A","type3B","type4","type5","type6A",
         "type6B","type6C","type7","type8","type9","type10","type11",
         "type12","type13","type14","type15","type16"]
    :param raactype: key of AAGroup # of ifeature.PseKRAAC.typeX
    :param subtype: a dictionary, for example: {'g-gap': 0, 'lambda-correlation': 4}
        Note: g-gap + lambda-correlation < 5 (smallest length of sequences of given fasta file)
    :param ktuple: int 1, 2, or 3 # of ifeature.PseKRAAC.typeX
    :param gap_lambda: int 1 # of ifeature.PseKRAAC.typeX
        gap value for the ‘g-gap’ model  or lambda value for the ‘lambda-correlation’ model, 10 values are available (i.e. 0, 1, 2, ..., 9)
    :return: dataframe of feature and the last column is the label of class with name "default"
    """
    fastas = readFasta.readFasta(path)
    eval_func = "%s.type1(fastas, subtype, raactype, ktuple, gap_lambda)" % (ft_name)
    print(eval_func)
    encdn = eval(eval_func)
    df = pd.DataFrame(encdn)
    df.index = df.iloc[:, 0]
    df.columns = df.iloc[0]
    df.drop(["#"], axis=1, inplace=True)
    df.drop(["#"], axis=0, inplace=True)
    print("feature number of PseKRAAC.%s(g-gap=%d, lambda-correlation=%d, raac_type=%d, ktuple=%d, gap_lambda=%d): %d" %
          (ft_name, subtype['g-gap'], subtype['lambda-correlation'], raactype, ktuple, gap_lambda, len(df.columns)))
    ft_whole_name = "%sraac%s" % (ft_name, raactype)
    print("Gene %s from path :\n\t%s" % (ft_whole_name, path))
    row, col = df.shape
    print("\t\t its class value: %s" % (str(class_val)))
    print("\t\t its input No.: %s" % row)
    print("\t\t its feature No.: %s" % col)
    # add class
    if class_val != None:
        df["default"] = [class_val] * len(fastas)
    return df


def GeneIfeature(path, ft_name="AAC", nlag=4, lambdaValue=4, class_val=1):
    """
    run codes as follows if you want try all features of ifeature.codes:
    from codes.ifeature.codes import *
    for ft_name in ft_whole_type:
        if ft_name in rm_type:
            continue
    :param path: given fasta file location
    :param ft_name: each element of a list ft_whole_type:
            ft_whole_type = ["AAC","EAAC","CKSAAP","DPC","DDE","TPC","BINARY","GAAC","EGAAC",
                "CKSAAGP","GDPC","GTPC","AAINDEX","ZSCALE","BLOSUM62","NMBroto",
                "Moran","Geary","CTDC","CTDT","CTDD","CTriad","KSCTriad",
                "SOCNumber","QSOrder","PAAC","APAAC","KNNprotein","KNNpeptide",
                "PSSM","SSEC","SSEB","Disorder","DisorderC","DisorderB","ASA","TA"]
            except the list rm_type as follows:
            rm_type = ["EAAC", "BINARY", "EGAAC", "AAINDEX", "ZSCALE", "BLOSUM62", "PSSM", "ASA", "TA", "Disorder", "DisorderB",
                   "DisorderC", "KNNprotein", "KNNpeptide", "SSEC", "SSEB"]
            because of errors when run ifeature.codes.ft_name:
                Error: for "EAAC"/"BINARY"/"EGAAC"/"AAINDEX"/"ZSCALE"/"BLOSUM62"/"PSSM"/"ASA"/"TA" encoding, the input fasta sequences should be with equal length.
                "KNNprotein"/"KNNpeptide": should have the train fasta file and a label file, do it later
                "SSEC"/"SSEB": secondary structure
                "Disorder"/"DisorderB"/"DisorderC" : Protein disorder information was first predicted by the VSL2 software
                "Disorder"/"DisorderB": encoding, the input fasta sequences should be with equal length.
    :return: dataframe of feature and the last column is the label of class with name "default"
    """
    fastas = readFasta.readFasta(path)
    eval_func = "%s.%s(fastas, order=None, nlag=%d, lambdaValue=%d, gap=1)" % (ft_name, ft_name, nlag, lambdaValue)
    print(eval_func)
    encdn = eval(eval_func)
    df = pd.DataFrame(encdn)
    df.index = df.iloc[:, 0]
    df.columns = df.iloc[0]
    df.drop(["#"], axis=1, inplace=True)
    df.drop(["#"], axis=0, inplace=True)
    # print("%s's feature number: %d" % (ft_name, len(df.columns)))
    print("Gene %s from path :\n\t%s" % (ft_name, path))
    row, col = df.shape
    print("\t\t its class value: %s" % (str(class_val)))
    print("\t\t its input No.: %s" % row)
    print("\t\t its feature No.: %s" % col)
    # add class
    if class_val != None:
        df["default"] = [class_val] * len(fastas)
    return df


def calMetrics(pred_rs):
    ori = pred_rs['default'].copy()
    pred = pred_rs['Label'].copy()
    Accuracy = check_metric(ori, pred, 'Accuracy')
    if 'Score_1' in pred_rs:
        sco = pred_rs['Score_1'].copy()
        AUC = check_metric(ori, sco, 'AUC')
    else:
        AUC = 0
    Recall = check_metric(ori, pred, 'Recall')
    Precision = check_metric(ori, pred, 'Precision')
    F1 = check_metric(ori, pred, 'F1')
    Kappa = check_metric(ori, pred, 'Kappa')
    rs = confusion_matrix(ori, pred)
    [tn, fp], [fn, tp] = rs[0], rs[1]
    if tp*tn - fp*fn == 0:
        MCC = 0
    else:
        MCC = check_metric(ori, pred, 'MCC')
    # MAE, MSE, RMSE, R2, RMSLE, MAPE.
    TNR = tn / (tn + fp)
    metrics_dict = {"Accuracy":Accuracy, "AUC":AUC, "Sp/TNR":TNR, "Recall/Sn/TPR":Recall, "Prec.":Precision, "F1":F1, "Kappa":Kappa, "MCC":MCC}
    metrics = pd.DataFrame([metrics_dict])
    return metrics

def SpecificityTNR(ori, pred):
    rs = confusion_matrix(ori, pred)
    [tn, fp], [fn, tp] = rs[0], rs[1]
    # tn / (tn + fp)
    return tn / (tn + fp)

from glob import glob
def getUnexistedName(file_path, file_type=".xlsx"):
    file_name = os.path.basename(file_path)
    dir_name = os.path.dirname(file_path)
    if not(os.path.isfile(file_path)):
        new_file = file_name
    else:
        name = file_name.split(file_type)[0]
        regex_str = name[:-1] + "*" + file_type
        regex_str = os.path.join(dir_name,regex_str)
        mylist = [f for f in glob(regex_str)]
        s = sorted(mylist)
        s = [os.path.basename(i) for i in s]
        s.remove(file_name)
        # print("file name:", file_name)
        # print(s)
        if len(s) > 0:
            nums = []
            for n in s:
                num_name = n.split(file_type)[0]
                # print(num_name)
                str_i = [str(i) for i in range(10)]
                str_num = num_name.split("-")[-1]
                a = [i in str_i for i in str_num]
                if len(str_num) == sum(a):
                    num = int(str_num)
                else:
                    s.remove(n)
                    continue
                nums.append(num)
            if len(nums) > 0:
                max_n = sorted(nums)[-1]
        if len(s) == 0:
            new_file = name + "-0" + file_type
        else:
            new_file = name + "-" + str(max_n+1) + file_type
    new_path = os.path.join(dir_name, new_file)
    return new_path

def getExistedLargestName(file_path, file_type=".xlsx"):
    file_name = os.path.basename(file_path)
    dir_name = os.path.dirname(file_path)
    if not(os.path.isfile(file_path)):
        new_file = file_name
        print("%s unexisted." % file_path)
    else:
        name = file_name.split(file_type)[0]
        regex_str = name[:-1] + "*" + file_type
        regex_str = os.path.join(dir_name,regex_str)
        mylist = [f for f in glob(regex_str)]
        s = sorted(mylist)
        s = [os.path.basename(i) for i in s]
        s.remove(file_name)
        if len(s) > 0:
            nums = []
            for n in s:
                num_name = n.split(file_type)[0]
                str_i = [str(i) for i in range(10)]
                str_num = num_name.split("-")[-1]
                a = [i in str_i for i in str_num]
                if len(str_num) == sum(a):
                    num = int(str_num)
                else:
                    s.remove(n)
                    continue
                    nums.append(num)
            if len(nums) > 0:
                max_n = sorted(nums)[-1]
        if len(s) == 0:
            new_file = file_name
        else:
            new_file = name + "-" + str(max_n) + file_type
    new_path = os.path.join(dir_name, new_file)
    return new_path

import dill
def write_pkl(obj, file_path):
    with open(file_path, 'wb') as f:
        dill.dump(obj, f)
    return

def read_pkl(file_path):
    with open(file_path, "rb") as f:
        final_rs = dill.load(f)
    return final_rs

def importDataByPsekraac(po_path, ne_path, ft_name="type1", raactype=2, subtype={'g-gap': 0, 'lambda-correlation': 4}, ktuple=2, gap_lambda=1):
    po_df = genePsekraac(po_path, ft_name, raactype, subtype, ktuple, gap_lambda, class_val=1)
    ne_df = genePsekraac(ne_path, ft_name, raactype, subtype, ktuple, gap_lambda, class_val=0)
    df = pd.concat((po_df,ne_df))
    return df

def importDataByiFtCodes(po_path, ne_path, ft_name="AAC", nlag=4, lambdaValue=4):
    po_df = GeneIfeature(po_path, ft_name, nlag, lambdaValue, class_val=1)
    ne_df = GeneIfeature(ne_path, ft_name, nlag, lambdaValue, class_val=0)
    df = pd.concat((po_df,ne_df))
    return df

def import1FtDataPoNe(po_path, ne_path, ft_whole_name="type1raac10"):
    if "type" in ft_whole_name:
        raactype = int(ft_whole_name.split("raac")[-1])
        if "Ktuple" in ft_whole_name:
            ft_name = ft_whole_name.split("Ktuple")[0]
        else:
            ft_name = ft_whole_name.split("raac")[0]
        df = importDataByPsekraac(po_path, ne_path, ft_name, raactype)
    else:
        ft_name = ft_whole_name
        df = importDataByiFtCodes(po_path, ne_path, ft_name)
    return df

def addZeroCols(df, ktuple=2):
    row, col = df.shape
    df = df.copy()
    p = ktuple
    add_col = int(np.power(np.ceil(np.power(col, 1/p)), p) - col)
    for i in range(add_col):
        col_name = "zero%d" % i
        df[col_name] = [0] * row
    return df

def MLftset2ftImgAndLabel(df):
    colname = df.columns.to_list()
    if "default" in colname:
        label = df["default"].to_list()
        new_df = df.drop(["default"], 1)
    else:
        label = None
        new_df = df
    dict = {"img": new_df.copy(),
            "cls": label}
    return dict

def CNNftset2ftImgAndLabel(df, ktuple=2):
    if "default" in df.columns.to_list():
        label = df["default"].to_list()
        row = len(df)
        label = np.array(label).reshape((row, 1))
        label = label.astype(dtype=np.uint8)
        new_df = df.drop(["default"], 1)
    else:
        label = None
        new_df = df.copy()
    # add zero columns to a square or 3 power
    new_df = addZeroCols(new_df, ktuple=ktuple)
    new_df = new_df.to_numpy()
    row, col = new_df.shape
    img_width = int(np.power(col,1/ktuple))
    if ktuple == 2:
        new_df = np.reshape(new_df, (row, img_width, img_width, 1))
    if ktuple == 3:
        new_df = np.reshape(new_df, (row, img_width, img_width, img_width))
    new_df = new_df.astype(dtype=np.float32)
    dict = {"img": new_df,
            "cls": label}
    return dict

def CNNimportFtsDataSetsPoNe(po_path, ne_path, ft_list):
    sets = {}
    for ft_whole_name in ft_list:
        df = import1FtDataPoNe(po_path, ne_path, ft_whole_name)
        dict = CNNftset2ftImgAndLabel(df)
        sets[ft_whole_name] = dict
    return sets

def import1FtData(path, ft_whole_name="type1raac10", subtype={'g-gap': 0, 'lambda-correlation': 4},
                  ktuple=2, gap_lambda=1, nlag=4, lambdaValue=4, class_val=None):
    if "type" in ft_whole_name:
        raactype = int(ft_whole_name.split("raac")[-1])
        if "Ktuple" in ft_whole_name:
            ft_name = ft_whole_name.split("Ktuple")[0]
        else:
            ft_name = ft_whole_name.split("raac")[0]
        df = genePsekraac(path, ft_name, raactype, subtype, ktuple, gap_lambda, class_val=class_val)
    else:
        ft_name = ft_whole_name
        df = GeneIfeature(path, ft_name, nlag, lambdaValue, class_val=class_val)
    return df

def CNNimportFtsDataSets(path, ft_list, class_val=None):
    sets = {}
    for ft_whole_name in ft_list:
        print("processing: %s" % ft_whole_name)
        df = import1FtData(path, ft_whole_name, class_val=class_val)
        dict = CNNftset2ftImgAndLabel(df)
        sets[ft_whole_name] = dict
    return sets

def MLstandardInputOutput(sets):
    c = -1
    for ft_type in sets:
        c += 1
        if c == 0:
            ft = sets[ft_type]['img']
        else:
            temp = sets[ft_type]['img']
            ft = pd.concat([ft, temp], axis=1)
    row, col = ft.shape
    col_names = ["ft%d" % i for i in range(col)]
    ft.columns = col_names
    if sets[ft_type]['cls'] != None:
        ft['default'] = sets[ft_type]['cls']
    return ft

def CNNstandardInputOutput(sets):
    X = []
    for ft_type in sets:
        x = sets[ft_type]['img']
        X.append(x)
    Y = sets[ft_type]['cls']
    return X, Y

def cnnArch(sets, hyperParas, opt):
    mdls, mdls_out, mdls_in = [], [], []
    for key in sets:
        in_shape = sets[key]["img"].shape[1:]
        model = models.Sequential()
        for l in range(hyperParas["layer_num"]):
            model.add(layers.BatchNormalization(input_shape=in_shape))
            model.add(layers.Conv2D(hyperParas["filter_num"], (3, 3), activation='relu', padding="same"))#, input_shape=in_shape))
            model.add(layers.MaxPooling2D((2, 2), padding="same"))
            model.add(layers.Dropout(hyperParas["dropOutRate"]))
        model.add(layers.Flatten())
        model.add(layers.Dense(64, activation='relu'))
        mdls.append(model)
        mdls_out.append(model.output)
        mdls_in.append(model.input)
    if len(sets.keys()) > 1:
        merge = layers.Add()(mdls_out)
        merge = layers.Dense(32, activation="relu")(merge)
        merge = layers.Dense(2, activation="sigmoid")(merge)
        newMdl = models.Model(mdls_in, merge)
    else:
        model.add(layers.Dense(32, activation="relu"))
        model.add(layers.Dense(2, activation="sigmoid"))
        newMdl = model
    newMdl.compile(optimizer=opt,
                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                   metrics=["accuracy"])
    CNNMdl = newMdl
    return CNNMdl

def DevelopfitAndSaveCNNMdl(sets, mdl_path, hist_path, hyperParas, lrParas):
    patience, monitor = lrParas["patience"], lrParas["monitor"]
    decay_rate, decay_steps = lrParas["decay_rate"], lrParas["decay_steps"]
    lr = lrParas["lr"]
    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience)  # , baseline=baseline)
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=lr,
                                                                   decay_steps=decay_steps, decay_rate=decay_rate)
    opt = keras.optimizers.Adam(learning_rate=lr_schedule)
    CNNMdl = cnnArch(sets, hyperParas, opt)
    X, Y = CNNstandardInputOutput(sets)
    history = CNNMdl.fit(X, Y, epochs=hyperParas["epoch"], validation_split=0.2, callbacks=[callback])
    CNNMdl.save(mdl_path)
    print("save CNN mdl to: %s" % mdl_path)
    write_pkl(history.history, hist_path)
    print("save CNN history to: %s" % hist_path)
    return CNNMdl

def predictSequenceFromSaveKerasMdl(test_path, Mdl_path, testRs_path, ft_list):
    sets = CNNimportFtsDataSets(test_path, ft_list, class_val=None)
    names, seqs, lens, records = readFastaYan(test_path)
    X, Y = CNNstandardInputOutput(sets)
    mdl_name = os.path.basename(mdl_path)
    CNNMdl = keras.models.load_model(Mdl_path)
    pred = CNNMdl.predict(X)
    df = pd.DataFrame(pred[:, 1], index=names, columns=[mdl_name.split(".")[0]])
    df.to_csv(testRs_path)
    print("save %s prediction to: %s" % (test_path, testRs_path))
    return Y, pred
